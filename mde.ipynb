{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e45c131",
   "metadata": {},
   "source": [
    "## Monocular Depth Estimation Project\n",
    "\n",
    "This file will follow the full training and evaluation process, report results across experiments, and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from DPT.dpt.models import DPTDepthModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataloaders.nyu_data import NyuDepthV2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from losses.mde_losses import ScaleAndShiftInvariantLoss\n",
    "from losses.LMR import LMRLoss\n",
    "from LNRegularizer.LNR import LNR\n",
    "from torchvision.transforms import v2\n",
    "from typing import Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# functions for displaying results\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7113efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "NYU_DATA_PATH = \"data/nyu_data/nyu_depth_v2_labeled.mat\"\n",
    "\n",
    "# download from http://horatio.cs.nyu.edu/mit/silberman/indoor_seg_sup/splits.mat\n",
    "NYU_SPLIT_PATH = \"data/nyu_data/splits.mat\"\n",
    "\n",
    "nyu_test_ds = NyuDepthV2(NYU_DATA_PATH, NYU_SPLIT_PATH, split=\"test\")\n",
    "nyu_train_ds = NyuDepthV2(NYU_DATA_PATH, NYU_SPLIT_PATH, split=\"train\")\n",
    "nyu_train_dataloader = DataLoader(nyu_train_ds, batch_size=12)\n",
    "nyu_test_dataloader = DataLoader(nyu_train_ds, batch_size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9709b9",
   "metadata": {},
   "source": [
    "## Training Functions\n",
    "\n",
    "Moving everything to a single notebook simplifies kernel issues and modifying portions of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optim: Optimizer,\n",
    "    epochs: int = 50,\n",
    "    print_every: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train depth head on NYU dataset with no additional regularization\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    pbar = tqdm(total=epochs * len(loader), desc=\"Training MDE:\")\n",
    "    i = 0\n",
    "    # loss function\n",
    "    loss = ScaleAndShiftInvariantLoss()\n",
    "\n",
    "    # training loop\n",
    "    for _ in range(epochs):\n",
    "        for batch in loader:\n",
    "            X = batch[\"image\"].float().to(\"mps\")\n",
    "            y = batch[\"depth\"].float().to(\"mps\")\n",
    "            mask = batch[\"mask\"].float().to(\"mps\")\n",
    "\n",
    "            X = X.permute(0, 3, 1, 2)\n",
    "\n",
    "            # calculate depth\n",
    "            prediction = model(X)\n",
    "\n",
    "            # calculate losses\n",
    "            err = loss(prediction, y, mask)\n",
    "            mse_loss = F.mse_loss(prediction, y)\n",
    "            l1_loss = F.smooth_l1_loss(prediction, y)\n",
    "\n",
    "            composite_loss = (2 * err) + (0.5 * mse_loss) + (0.1 * l1_loss)\n",
    "\n",
    "            # print(f\"composite_loss requires_grad: {composite_loss.requires_grad}\")\n",
    "\n",
    "            # process optimizer\n",
    "            optim.zero_grad()\n",
    "            composite_loss.backward()  # back-prop losses\n",
    "            optim.step()\n",
    "\n",
    "            # debugging\n",
    "            grads = []\n",
    "            for name, p in model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    grads.append(p.grad.norm().item())\n",
    "                else:\n",
    "                    pass\n",
    "                    # print(f\"{name} gradient is none\")\n",
    "\n",
    "            grads = torch.Tensor(grads)\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    mse_loss = F.mse_loss(prediction, y)\n",
    "                pbar.set_postfix_str(\n",
    "                    f\"train_loss: {err:.2f} | mse_loss: {mse_loss:.2f} | l1_loss: {l1_loss:.2f} | composite loss: {composite_loss:.2f} | Average gradient: {grads.mean()} | min grad: {grads.min()} | max grad {grads.max().item():.2f} | min pred. depth: {prediction.min().item():.2f} | max pred. depth: {prediction.max().item():.2f}\"\n",
    "                )\n",
    "                pbar.update(print_every)\n",
    "\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea09fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lmr(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optim: Optimizer,\n",
    "    epochs: int = 50,\n",
    "    print_every: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train depth head on NYU dataset with lmr regularizer\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    pbar = tqdm(total=epochs * len(loader), desc=\"Training MDE:\")\n",
    "    i = 0\n",
    "    # loss function\n",
    "    loss = ScaleAndShiftInvariantLoss()\n",
    "    lmr_loss = LMRLoss()\n",
    "\n",
    "    # training loop\n",
    "    for _ in range(epochs):\n",
    "        for batch in loader:\n",
    "            X = batch[\"image\"].float().to(\"mps\")\n",
    "            y = batch[\"depth\"].float().to(\"mps\")\n",
    "            mask = batch[\"mask\"].float().to(\"mps\")\n",
    "\n",
    "            X = X.permute(0, 3, 1, 2)\n",
    "\n",
    "            # pass input through LMR model\n",
    "            output_mask = LNR(X)\n",
    "\n",
    "            # calculate depth\n",
    "            prediction = model(X)\n",
    "\n",
    "            # calculate losses\n",
    "            err = loss(prediction, y, mask)\n",
    "            lmr_mask_loss = lmr_loss(\n",
    "                net_mask=output_mask, depth_hat=prediction.detach(), depth=y, k=100\n",
    "            )\n",
    "\n",
    "            err = err + lmr_mask_loss  # combine losses\n",
    "\n",
    "            # process optimizer\n",
    "            optim.zero_grad()\n",
    "            err.backward()  # back-prop losses\n",
    "            optim.step()\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    mse_loss = F.mse_loss(prediction, y)\n",
    "                pbar.set_postfix_str(\n",
    "                    f\"train_loss: {err:.2f} | mse_loss: {mse_loss:.2f}\"\n",
    "                )\n",
    "                pbar.update(print_every)\n",
    "\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_cutmix(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optim: Optimizer,\n",
    "    epochs: int = 50,\n",
    "    print_every: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train depth head on NYU dataset using cutmix regularization\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    pbar = tqdm(total=epochs * len(loader), desc=\"Training MDE:\")\n",
    "    i = 0\n",
    "    # loss function\n",
    "    loss = ScaleAndShiftInvariantLoss()\n",
    "    lmr_loss = LMRLoss()\n",
    "\n",
    "    # cutmix transform\n",
    "    cutmix = v2.CutMix()\n",
    "\n",
    "    # training loop\n",
    "    for _ in range(epochs):\n",
    "        for batch in loader:\n",
    "            X = batch[\"image\"].float().to(\"mps\")\n",
    "            y = batch[\"depth\"].float().to(\"mps\")\n",
    "            mask = batch[\"mask\"].float().to(\"mps\")\n",
    "\n",
    "            X = X.permute(0, 3, 1, 2)\n",
    "\n",
    "            # apply cutmix to batch\n",
    "            X, y = cutmix(X, y)\n",
    "\n",
    "            # calculate depth\n",
    "            prediction = model(X)\n",
    "\n",
    "            # calculate losses\n",
    "            err = loss(prediction, y, mask)\n",
    "            # process optimizer\n",
    "            optim.zero_grad()\n",
    "            err.backward()  # back-prop losses\n",
    "            optim.step()\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    mse_loss = F.mse_loss(prediction, y)\n",
    "                pbar.set_postfix_str(\n",
    "                    f\"train_loss: {err:.2f} | mse_loss: {mse_loss:.2f}\"\n",
    "                )\n",
    "                pbar.update(print_every)\n",
    "\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6817c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model: nn.Module, loader: DataLoader) -> Dict:\n",
    "    \"\"\"\n",
    "    assess the accuracy of the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pbar = tqdm(total=len(loader), desc=\"Evaluating MDE:\")\n",
    "    test_err = []\n",
    "    for batch in loader:\n",
    "        X = batch[\"image\"].float().to(\"mps\")\n",
    "        y = batch[\"depth\"].float().to(\"mps\")\n",
    "        with torch.no_grad():\n",
    "            X = X.permute(0, 3, 1, 2)\n",
    "            prediction = model(X)\n",
    "            err = F.mse_loss(\n",
    "                prediction, y\n",
    "            )  # when looking at error for evaluation use MSE loss\n",
    "            test_err.append(err)\n",
    "\n",
    "        # process optimizer\n",
    "        pbar.set_postfix_str(f\"mse_loss: {err:.2f}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "    print(f\"Average MSE Loss: {sum(test_err)/len(test_err)}\")\n",
    "    return {\"mse_avg\": sum(test_err) / len(test_err)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_frames(model: nn.Module, dataset: Dataset, indices: List[int]) -> None:\n",
    "    \"\"\"Generate a plot of depth images at specific indices\"\"\"\n",
    "    for i in indices:\n",
    "        datapoint = dataset[i]\n",
    "        X = datapoint[\"image\"]\n",
    "        y = datapoint[\"depth\"]\n",
    "        mask = datapoint[\"mask\"]\n",
    "\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 8))\n",
    "        ax1.imshow(X)\n",
    "        ax1.set_title(\"Image\")\n",
    "        ax2.imshow(y)\n",
    "        ax2.set_title(\"Truth depth\")\n",
    "\n",
    "        X = torch.Tensor(X).to(\"mps\").unsqueeze(0).permute(0, 3, 1, 2)\n",
    "        with torch.no_grad():\n",
    "            prediction = model(X).permute(1, 2, 0).cpu().numpy()\n",
    "        ax3.imshow(prediction, cmap=\"viridis\")\n",
    "        # ax3.title(\"Predicted depth\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea838c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model = (\n",
    "        DPTDepthModel(\n",
    "            path=\"/Users/michael/Documents/Grad_School/Fall25/DeepLearning/Project/MDE/DPT/dpt/weights/dpt_hybrid-midas-501f0c75.pt\",\n",
    "            scale=0.000305,\n",
    "            shift=0.1378,\n",
    "            invert=True,\n",
    "            backbone=\"vitb_rn50_384\",\n",
    "            non_negative=True,\n",
    "            enable_attention_hooks=False,\n",
    "        )\n",
    "        .float()\n",
    "        .to(\"mps\")\n",
    "    )\n",
    "\n",
    "    # keep everything but initialize the final head model\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"output_conv\" in name or \"head\" in name:\n",
    "            if \"weight\" in name:\n",
    "                print(f\"Initializing {name} to kaiming normal\")\n",
    "                param = nn.init.kaiming_normal(param)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9a0b7",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Each experiment will use a separate model so that the results can be compared directly in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ebf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = init_model()\n",
    "plot_test_frames(simple_model, nyu_test_ds, indices=[1, 3, 5])\n",
    "\n",
    "optim = Adam(simple_model.parameters(), lr=1e-4)\n",
    "\n",
    "# standard training - no regularization at all\n",
    "train_simple(\n",
    "    model=simple_model,\n",
    "    loader=nyu_train_dataloader,\n",
    "    optim=optim,\n",
    "    epochs=1,\n",
    ")\n",
    "simple_res = eval(simple_model, nyu_test_dataloader)\n",
    "plot_test_frames(simple_model, nyu_test_ds, indices=[1, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f154840",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutmix_model = init_model()\n",
    "optim = Adam(cutmix_model.parameters(), lr=1e-4)\n",
    "\n",
    "# training of cutmix model\n",
    "train_with_cutmix(\n",
    "    model=cutmix_model, loader=nyu_train_dataloader, optim=optim, epochs=1\n",
    ")\n",
    "cutmix_res = eval(cutmix_model, nyu_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with LMR regularization\n",
    "lmr_model = init_model()\n",
    "optim = Adam(cutmix_model.parameters(), lr=1e-4)\n",
    "\n",
    "# training of cutmix model\n",
    "train_with_lmr(model=lmr_model, loader=nyu_train_dataloader, optim=optim, epochs=1)\n",
    "lmr_res = eval(lmr_model, nyu_test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
